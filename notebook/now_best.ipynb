{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This variable is used by helperbot to make the training deterministic\n",
    "os.environ[\"SEED\"] = \"828\"\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "# from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n",
    "\n",
    "# from helperbot import (\n",
    "#     TriangularLR, BaseBot, WeightDecayOptimizerWrapper,\n",
    "#     GradualWarmupScheduler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(df):\n",
    "    df[\"Neither\"] = 0\n",
    "    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "    df[\"target\"] = 0\n",
    "    df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "    print(df.target.value_counts())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([\n",
    "    pd.read_csv(\"../input/gap-test.tsv\", delimiter=\"\\t\"),\n",
    "    pd.read_csv(\"../input/gap-validation.tsv\", delimiter=\"\\t\")\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../input/gap-development.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1105\n",
      "1    1060\n",
      "2     289\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = extract_target(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    925\n",
      "0    874\n",
      "2    201\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test = extract_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\n",
    "assert sample_sub.shape[0] == df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 344314.08B/s]\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL = 'bert-large-uncased'\n",
    "CASED = False\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    BERT_MODEL,\n",
    "    do_lower_case=CASED,\n",
    "    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row, tokenizer):\n",
    "    break_points = sorted(\n",
    "        [\n",
    "            (\"A\", row[\"A-offset\"], row[\"A\"]),\n",
    "            (\"B\", row[\"B-offset\"], row[\"B\"]),\n",
    "            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n",
    "        ], key=lambda x: x[0]\n",
    "    )\n",
    "    tokens, spans, current_pos = [], {}, 0\n",
    "    for name, offset, text in break_points:\n",
    "        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "        # Make sure we do not get it wrong\n",
    "        assert row[\"Text\"][offset:offset+len(text)] == text\n",
    "        # Tokenize the target\n",
    "        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n",
    "        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n",
    "        tokens.extend(tmp_tokens)\n",
    "        current_pos = offset + len(text)\n",
    "    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "    assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n",
    "\n",
    "def collate_examples(batch, truncate_len=490):\n",
    "    \"\"\"Batch preparation.\n",
    "    \n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"    \n",
    "    transposed = list(zip(*batch))\n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    for i, row in enumerate(transposed[0]):\n",
    "        row = np.array(row[:truncate_len])\n",
    "        tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.from_numpy(tokens)\n",
    "    # Offsets\n",
    "    offsets = torch.stack([\n",
    "        torch.LongTensor(x) for x in transposed[1]\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    # Labels\n",
    "    if len(transposed) == 2:\n",
    "        return token_tensor, offsets, None\n",
    "    labels = torch.LongTensor(transposed[2])\n",
    "    return token_tensor, offsets, labels\n",
    "\n",
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            self.y = df.target.values.astype(\"uint8\")\n",
    "        \n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            tokens, offsets = tokenize(row, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = GAPDataset(df_test, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=False)\n",
    "        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248501532/1248501532 [03:07<00:00, 6674702.69B/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bd740d8fff09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mset_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-6b1af5f69aab>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert_model, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported BERT model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Head' is not defined"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=191)\n",
    "\n",
    "val_preds, test_preds, val_ys, val_losses = [], [], [], []\n",
    "for train_index, valid_index in skf.split(df_train, df_train[\"target\"]):\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Fold \" + str(len(val_preds)))\n",
    "    print(\"=\" * 20)\n",
    "    train_ds = GAPDataset(df_train.iloc[train_index], tokenizer)\n",
    "    val_ds = GAPDataset(df_train.iloc[valid_index], tokenizer)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=128,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n",
    "    # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n",
    "    set_trainable(model.bert, False)\n",
    "    set_trainable(model.head, True)\n",
    "    optimizer = WeightDecayOptimizerWrapper(\n",
    "        torch.optim.Adam(model.parameters(), lr=2e-3),\n",
    "        0.05\n",
    "    )\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    bot = GAPBot(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer=optimizer, echo=True,\n",
    "        avg_window=40\n",
    "    )\n",
    "    gc.collect()\n",
    "    steps_per_epoch = len(train_loader) \n",
    "    n_steps = steps_per_epoch * 15\n",
    "    bot.train(\n",
    "        n_steps,\n",
    "        log_interval=steps_per_epoch // 2,\n",
    "        snapshot_interval=steps_per_epoch,\n",
    "#         scheduler=GradualWarmupScheduler(optimizer, 20, int(steps_per_epoch * 4),\n",
    "#             after_scheduler=CosineAnnealingLR(\n",
    "#                 optimizer, n_steps - int(steps_per_epoch * 4)\n",
    "#             )\n",
    "#         )\n",
    "        scheduler=TriangularLR(\n",
    "            optimizer, 20, ratio=3, steps_per_cycle=n_steps)\n",
    "    )\n",
    "    # Load the best checkpoint\n",
    "    bot.load_model(bot.best_performers[0][1])\n",
    "    bot.remove_checkpoints(keep=0)    \n",
    "    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
    "    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n",
    "    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n",
    "    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n",
    "    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
