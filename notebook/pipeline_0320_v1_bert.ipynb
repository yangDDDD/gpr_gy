{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n",
    "\n",
    "from helperbot import (\n",
    "    TriangularLR, BaseBot, WeightDecayOptimizerWrapper,\n",
    "    GradualWarmupScheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(df):\n",
    "    df[\"Neither\"] = 0\n",
    "    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "    df[\"target\"] = 0\n",
    "    df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "    print(df.target.value_counts())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([\n",
    "    pd.read_csv(\"../input/gap-test.tsv\", delimiter=\"\\t\"),\n",
    "    pd.read_csv(\"../input/gap-validation.tsv\", delimiter=\"\\t\")\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../input/gap-development.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1105\n",
      "1    1060\n",
      "2     289\n",
      "Name: target, dtype: int64\n",
      "1    925\n",
      "0    874\n",
      "2    201\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = extract_target(df_train)\n",
    "df_test = extract_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\n",
    "assert sample_sub.shape[0] == df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-large-uncased'\n",
    "CASED = False\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    BERT_MODEL,\n",
    "    do_lower_case=CASED,\n",
    "    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            self.y = df.target.values.astype(\"uint8\")\n",
    "        \n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            tokens, offsets = tokenize(row, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row, tokenizer):\n",
    "    break_points = sorted(\n",
    "        [\n",
    "            (\"A\", row[\"A-offset\"], row[\"A\"]),\n",
    "            (\"B\", row[\"B-offset\"], row[\"B\"]),\n",
    "            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n",
    "        ], key=lambda x: x[0]\n",
    "    )\n",
    "    tokens, spans, current_pos = [], {}, 0\n",
    "    for name, offset, text in break_points:\n",
    "        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "        # Make sure we do not get it wrong\n",
    "        assert row[\"Text\"][offset:offset+len(text)] == text\n",
    "        # Tokenize the target\n",
    "        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n",
    "        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n",
    "        tokens.extend(tmp_tokens)\n",
    "        current_pos = offset + len(text)\n",
    "    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "    assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_examples(batch, truncate_len=490):\n",
    "    \"\"\"Batch preparation.\n",
    "    \n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"    \n",
    "    transposed = list(zip(*batch))\n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    for i, row in enumerate(transposed[0]):\n",
    "        row = np.array(row[:truncate_len])\n",
    "        tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.from_numpy(tokens)\n",
    "    # Offsets\n",
    "    offsets = torch.stack([\n",
    "        torch.LongTensor(x) for x in transposed[1]\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    # Labels\n",
    "    if len(transposed) == 2:\n",
    "        return token_tensor, offsets, None\n",
    "    labels = torch.LongTensor(transposed[2])\n",
    "    return token_tensor, offsets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = GAPDataset(df_test, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "#         self.span_extractor = SelfAttentiveSpanExtractor(bert_hidden_size)\n",
    "        self.span_extractor = EndpointSpanExtractor(\n",
    "            bert_hidden_size, \"x,y,x*y\"\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 7),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(bert_hidden_size * 7, 48),           \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(48),             \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(48, 3)\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                print(\"Initing batchnorm\")\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    print(\"Initing linear with weight normalization\")\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    print(\"Initing linear\")\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.size(2) == self.bert_hidden_size\n",
    "        spans_contexts = self.span_extractor(\n",
    "            bert_outputs, \n",
    "            offsets[:, :4].reshape(-1, 2, 2)\n",
    "        ).reshape(offsets.size()[0], -1)\n",
    "        return self.fc(torch.cat([\n",
    "            spans_contexts,\n",
    "            torch.gather(\n",
    "                bert_outputs, 1,\n",
    "                offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n",
    "            ).squeeze(1)\n",
    "        ], dim=1))\n",
    "\n",
    "\n",
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=False)\n",
    "        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from fast.ai library\n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPBot(BaseBot):\n",
    "    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n",
    "        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n",
    "        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n",
    "        device=\"cuda:0\", use_tensorboard=False):\n",
    "        super().__init__(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer=optimizer, clip_grad=clip_grad,\n",
    "            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n",
    "            batch_idx=batch_idx, echo=echo,\n",
    "            device=device, use_tensorboard=use_tensorboard\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_format = \"%.6f\"\n",
    "        \n",
    "    def extract_prediction(self, tensor):\n",
    "        return tensor\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n",
    "        loss = self.eval(self.val_loader)\n",
    "        loss_str = self.loss_format % loss\n",
    "        self.logger.info(\"Snapshot loss %s\", loss_str)\n",
    "        self.logger.tb_scalars(\n",
    "            \"losses\", {\"val\": loss},  self.step)\n",
    "        target_path = (\n",
    "            self.checkpoint_dir / \"best.pth\")        \n",
    "        if not self.best_performers or (self.best_performers[0][0] > loss):\n",
    "            torch.save(self.model.state_dict(), target_path)\n",
    "            self.best_performers = [(loss, target_path, self.step)]\n",
    "        self.logger.info(\"Saving checkpoint %s...\", target_path)\n",
    "        assert Path(target_path).exists()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 0\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 09:14:31 PM]] SEED: 9293\n",
      "[[03/22/2019 09:14:31 PM]] # of paramters: 335,500,579\n",
      "[[03/22/2019 09:14:31 PM]] # of trainable paramters: 358,691\n",
      "[[03/22/2019 09:14:32 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/22/2019 09:14:32 PM]] Batches per epoch: 61\n",
      "[[03/22/2019 09:14:32 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 09:14:58 PM]] Step 30: train 1.584353 lr: 3.333e-04\n",
      "[[03/22/2019 09:15:29 PM]] Step 60: train 1.388470 lr: 5.833e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  2.02s/it]\n",
      "[[03/22/2019 09:15:47 PM]] Snapshot loss 0.790815\n",
      "[[03/22/2019 09:15:48 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:15:48 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:15:48 PM]] ====================Epoch 2====================\n",
      "[[03/22/2019 09:16:19 PM]] Step 90: train 1.236052 lr: 8.333e-04\n",
      "[[03/22/2019 09:16:53 PM]] Step 120: train 1.159770 lr: 1.083e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.31s/it]\n",
      "[[03/22/2019 09:17:15 PM]] Snapshot loss 0.723812\n",
      "[[03/22/2019 09:17:23 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:17:23 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:17:23 PM]] ====================Epoch 3====================\n",
      "[[03/22/2019 09:17:52 PM]] Step 150: train 1.087198 lr: 1.333e-03\n",
      "[[03/22/2019 09:18:27 PM]] Step 180: train 1.028516 lr: 1.583e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 09:18:49 PM]] Snapshot loss 0.685580\n",
      "[[03/22/2019 09:18:57 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:18:57 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:18:57 PM]] ====================Epoch 4====================\n",
      "[[03/22/2019 09:19:27 PM]] Step 210: train 0.979745 lr: 1.833e-03\n",
      "[[03/22/2019 09:20:01 PM]] Step 240: train 0.941850 lr: 1.972e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:20:25 PM]] Snapshot loss 0.629978\n",
      "[[03/22/2019 09:20:33 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:20:33 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:20:33 PM]] ====================Epoch 5====================\n",
      "[[03/22/2019 09:20:59 PM]] Step 270: train 0.900989 lr: 1.889e-03\n",
      "[[03/22/2019 09:21:34 PM]] Step 300: train 0.873233 lr: 1.806e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:22:00 PM]] Snapshot loss 0.616562\n",
      "[[03/22/2019 09:22:07 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:22:07 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:22:07 PM]] ====================Epoch 6====================\n",
      "[[03/22/2019 09:22:35 PM]] Step 330: train 0.770321 lr: 1.723e-03\n",
      "[[03/22/2019 09:23:08 PM]] Step 360: train 0.707171 lr: 1.640e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.36s/it]\n",
      "[[03/22/2019 09:23:36 PM]] Snapshot loss 0.626184\n",
      "[[03/22/2019 09:23:36 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:23:36 PM]] ====================Epoch 7====================\n",
      "[[03/22/2019 09:24:03 PM]] Step 390: train 0.665948 lr: 1.557e-03\n",
      "[[03/22/2019 09:24:40 PM]] Step 420: train 0.623190 lr: 1.475e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.33s/it]\n",
      "[[03/22/2019 09:25:07 PM]] Snapshot loss 0.599021\n",
      "[[03/22/2019 09:25:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:25:15 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:25:15 PM]] ====================Epoch 8====================\n",
      "[[03/22/2019 09:25:37 PM]] Step 450: train 0.591220 lr: 1.392e-03\n",
      "[[03/22/2019 09:26:13 PM]] Step 480: train 0.562361 lr: 1.309e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.38s/it]\n",
      "[[03/22/2019 09:26:43 PM]] Snapshot loss 0.586243\n",
      "[[03/22/2019 09:26:51 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:26:51 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:26:51 PM]] ====================Epoch 9====================\n",
      "[[03/22/2019 09:27:16 PM]] Step 510: train 0.541465 lr: 1.226e-03\n",
      "[[03/22/2019 09:27:49 PM]] Step 540: train 0.519657 lr: 1.143e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:28:19 PM]] Snapshot loss 0.608784\n",
      "[[03/22/2019 09:28:19 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:28:19 PM]] ====================Epoch 10====================\n",
      "[[03/22/2019 09:28:42 PM]] Step 570: train 0.507779 lr: 1.060e-03\n",
      "[[03/22/2019 09:29:14 PM]] Step 600: train 0.486235 lr: 9.767e-04\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.34s/it]\n",
      "[[03/22/2019 09:29:46 PM]] Snapshot loss 0.589636\n",
      "[[03/22/2019 09:29:46 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:29:46 PM]] ====================Epoch 11====================\n",
      "[[03/22/2019 09:30:08 PM]] Step 630: train 0.470416 lr: 8.937e-04\n",
      "[[03/22/2019 09:30:44 PM]] Step 660: train 0.454399 lr: 8.108e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:31:15 PM]] Snapshot loss 0.593414\n",
      "[[03/22/2019 09:31:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:31:15 PM]] ====================Epoch 12====================\n",
      "[[03/22/2019 09:31:36 PM]] Step 690: train 0.443476 lr: 7.278e-04\n",
      "[[03/22/2019 09:32:09 PM]] Step 720: train 0.433735 lr: 6.448e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:32:43 PM]] Snapshot loss 0.588582\n",
      "[[03/22/2019 09:32:43 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:32:43 PM]] ====================Epoch 13====================\n",
      "[[03/22/2019 09:33:04 PM]] Step 750: train 0.423429 lr: 5.619e-04\n",
      "[[03/22/2019 09:33:38 PM]] Step 780: train 0.417510 lr: 4.789e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:34:12 PM]] Snapshot loss 0.590001\n",
      "[[03/22/2019 09:34:12 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:34:12 PM]] ====================Epoch 14====================\n",
      "[[03/22/2019 09:34:31 PM]] Step 810: train 0.404055 lr: 3.959e-04\n",
      "[[03/22/2019 09:35:07 PM]] Step 840: train 0.392817 lr: 3.130e-04\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.39s/it]\n",
      "[[03/22/2019 09:35:42 PM]] Snapshot loss 0.577233\n",
      "[[03/22/2019 09:35:49 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:35:49 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:35:49 PM]] ====================Epoch 15====================\n",
      "[[03/22/2019 09:36:07 PM]] Step 870: train 0.384273 lr: 2.300e-04\n",
      "[[03/22/2019 09:36:40 PM]] Step 900: train 0.376586 lr: 1.470e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:37:15 PM]] Snapshot loss 0.583643\n",
      "[[03/22/2019 09:37:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 09:37:36 PM]] Confirm val loss: 0.5772\n",
      "100%|██████████| 16/16 [01:27<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 09:39:31 PM]] SEED: 9293\n",
      "[[03/22/2019 09:39:31 PM]] # of paramters: 335,500,579\n",
      "[[03/22/2019 09:39:31 PM]] # of trainable paramters: 358,691\n",
      "[[03/22/2019 09:39:31 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/22/2019 09:39:31 PM]] Batches per epoch: 61\n",
      "[[03/22/2019 09:39:31 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 09:40:02 PM]] Step 30: train 1.737108 lr: 3.333e-04\n",
      "[[03/22/2019 09:40:35 PM]] Step 60: train 1.562890 lr: 5.833e-04\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.59s/it]\n",
      "[[03/22/2019 09:40:56 PM]] Snapshot loss 0.869670\n",
      "[[03/22/2019 09:40:57 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:40:57 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:40:58 PM]] ====================Epoch 2====================\n",
      "[[03/22/2019 09:41:31 PM]] Step 90: train 1.405517 lr: 8.333e-04\n",
      "[[03/22/2019 09:42:04 PM]] Step 120: train 1.309680 lr: 1.083e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.70s/it]\n",
      "[[03/22/2019 09:42:28 PM]] Snapshot loss 0.677972\n",
      "[[03/22/2019 09:42:36 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:42:36 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:42:36 PM]] ====================Epoch 3====================\n",
      "[[03/22/2019 09:43:09 PM]] Step 150: train 1.201270 lr: 1.333e-03\n",
      "[[03/22/2019 09:43:43 PM]] Step 180: train 1.144164 lr: 1.583e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.57s/it]\n",
      "[[03/22/2019 09:44:07 PM]] Snapshot loss 0.616967\n",
      "[[03/22/2019 09:44:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:44:15 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:44:15 PM]] ====================Epoch 4====================\n",
      "[[03/22/2019 09:44:45 PM]] Step 210: train 1.082509 lr: 1.833e-03\n",
      "[[03/22/2019 09:45:19 PM]] Step 240: train 1.032128 lr: 1.972e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.63s/it]\n",
      "[[03/22/2019 09:45:44 PM]] Snapshot loss 0.620890\n",
      "[[03/22/2019 09:45:44 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:45:44 PM]] ====================Epoch 5====================\n",
      "[[03/22/2019 09:46:13 PM]] Step 270: train 0.988856 lr: 1.889e-03\n",
      "[[03/22/2019 09:46:47 PM]] Step 300: train 0.952550 lr: 1.806e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.55s/it]\n",
      "[[03/22/2019 09:47:12 PM]] Snapshot loss 0.587445\n",
      "[[03/22/2019 09:47:20 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:47:20 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:47:20 PM]] ====================Epoch 6====================\n",
      "[[03/22/2019 09:47:49 PM]] Step 330: train 0.833207 lr: 1.723e-03\n",
      "[[03/22/2019 09:48:21 PM]] Step 360: train 0.751109 lr: 1.640e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.58s/it]\n",
      "[[03/22/2019 09:48:48 PM]] Snapshot loss 0.563874\n",
      "[[03/22/2019 09:48:56 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:48:56 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:48:56 PM]] ====================Epoch 7====================\n",
      "[[03/22/2019 09:49:23 PM]] Step 390: train 0.695049 lr: 1.557e-03\n",
      "[[03/22/2019 09:49:59 PM]] Step 420: train 0.644072 lr: 1.475e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.68s/it]\n",
      "[[03/22/2019 09:50:27 PM]] Snapshot loss 0.567133\n",
      "[[03/22/2019 09:50:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:50:27 PM]] ====================Epoch 8====================\n",
      "[[03/22/2019 09:50:53 PM]] Step 450: train 0.616982 lr: 1.392e-03\n",
      "[[03/22/2019 09:51:27 PM]] Step 480: train 0.585933 lr: 1.309e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.66s/it]\n",
      "[[03/22/2019 09:51:56 PM]] Snapshot loss 0.560915\n",
      "[[03/22/2019 09:52:04 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:52:04 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:52:05 PM]] ====================Epoch 9====================\n",
      "[[03/22/2019 09:52:29 PM]] Step 510: train 0.558519 lr: 1.226e-03\n",
      "[[03/22/2019 09:53:03 PM]] Step 540: train 0.538140 lr: 1.143e-03\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.61s/it]\n",
      "[[03/22/2019 09:53:34 PM]] Snapshot loss 0.547018\n",
      "[[03/22/2019 09:53:42 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:53:42 PM]] New low\n",
      "\n",
      "[[03/22/2019 09:53:42 PM]] ====================Epoch 10====================\n",
      "[[03/22/2019 09:54:05 PM]] Step 570: train 0.518346 lr: 1.060e-03\n",
      "[[03/22/2019 09:54:39 PM]] Step 600: train 0.502703 lr: 9.767e-04\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.57s/it]\n",
      "[[03/22/2019 09:55:12 PM]] Snapshot loss 0.561113\n",
      "[[03/22/2019 09:55:12 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:55:12 PM]] ====================Epoch 11====================\n",
      "[[03/22/2019 09:55:35 PM]] Step 630: train 0.492689 lr: 8.937e-04\n",
      "[[03/22/2019 09:56:09 PM]] Step 660: train 0.472811 lr: 8.108e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.55s/it]\n",
      "[[03/22/2019 09:56:40 PM]] Snapshot loss 0.566589\n",
      "[[03/22/2019 09:56:40 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:56:40 PM]] ====================Epoch 12====================\n",
      "[[03/22/2019 09:57:01 PM]] Step 690: train 0.459857 lr: 7.278e-04\n",
      "[[03/22/2019 09:57:36 PM]] Step 720: train 0.445776 lr: 6.448e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.56s/it]\n",
      "[[03/22/2019 09:58:09 PM]] Snapshot loss 0.564576\n",
      "[[03/22/2019 09:58:09 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:58:09 PM]] ====================Epoch 13====================\n",
      "[[03/22/2019 09:58:29 PM]] Step 750: train 0.431967 lr: 5.619e-04\n",
      "[[03/22/2019 09:59:02 PM]] Step 780: train 0.417617 lr: 4.789e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.51s/it]\n",
      "[[03/22/2019 09:59:36 PM]] Snapshot loss 0.563798\n",
      "[[03/22/2019 09:59:36 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 09:59:36 PM]] ====================Epoch 14====================\n",
      "[[03/22/2019 09:59:54 PM]] Step 810: train 0.411633 lr: 3.959e-04\n",
      "[[03/22/2019 10:00:31 PM]] Step 840: train 0.399493 lr: 3.130e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.55s/it]\n",
      "[[03/22/2019 10:01:05 PM]] Snapshot loss 0.567681\n",
      "[[03/22/2019 10:01:05 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:01:05 PM]] ====================Epoch 15====================\n",
      "[[03/22/2019 10:01:24 PM]] Step 870: train 0.395342 lr: 2.300e-04\n",
      "[[03/22/2019 10:01:59 PM]] Step 900: train 0.383488 lr: 1.470e-04\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.69s/it]\n",
      "[[03/22/2019 10:02:37 PM]] Snapshot loss 0.570625\n",
      "[[03/22/2019 10:02:37 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 8/8 [00:20<00:00,  2.69s/it]\n",
      "[[03/22/2019 10:03:00 PM]] Confirm val loss: 0.5470\n",
      "100%|██████████| 16/16 [01:30<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 2\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:04:58 PM]] SEED: 9293\n",
      "[[03/22/2019 10:04:58 PM]] # of paramters: 335,500,579\n",
      "[[03/22/2019 10:04:58 PM]] # of trainable paramters: 358,691\n",
      "[[03/22/2019 10:04:58 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/22/2019 10:04:58 PM]] Batches per epoch: 61\n",
      "[[03/22/2019 10:04:58 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:05:29 PM]] Step 30: train 1.873517 lr: 3.333e-04\n",
      "[[03/22/2019 10:06:05 PM]] Step 60: train 1.663907 lr: 5.833e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:06:26 PM]] Snapshot loss 0.869567\n",
      "[[03/22/2019 10:06:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:06:27 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:06:27 PM]] ====================Epoch 2====================\n",
      "[[03/22/2019 10:07:00 PM]] Step 90: train 1.505145 lr: 8.333e-04\n",
      "[[03/22/2019 10:07:34 PM]] Step 120: train 1.401263 lr: 1.083e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:07:55 PM]] Snapshot loss 0.691355\n",
      "[[03/22/2019 10:08:03 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:08:03 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:08:03 PM]] ====================Epoch 3====================\n",
      "[[03/22/2019 10:08:35 PM]] Step 150: train 1.296315 lr: 1.333e-03\n",
      "[[03/22/2019 10:09:09 PM]] Step 180: train 1.225661 lr: 1.583e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.37s/it]\n",
      "[[03/22/2019 10:09:32 PM]] Snapshot loss 0.621809\n",
      "[[03/22/2019 10:09:40 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:09:40 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:09:40 PM]] ====================Epoch 4====================\n",
      "[[03/22/2019 10:10:09 PM]] Step 210: train 1.148932 lr: 1.833e-03\n",
      "[[03/22/2019 10:10:43 PM]] Step 240: train 1.092479 lr: 1.972e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:11:07 PM]] Snapshot loss 0.656836\n",
      "[[03/22/2019 10:11:07 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:11:07 PM]] ====================Epoch 5====================\n",
      "[[03/22/2019 10:11:39 PM]] Step 270: train 1.043791 lr: 1.889e-03\n",
      "[[03/22/2019 10:12:11 PM]] Step 300: train 1.001488 lr: 1.806e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:12:36 PM]] Snapshot loss 0.609815\n",
      "[[03/22/2019 10:12:44 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:12:44 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:12:44 PM]] ====================Epoch 6====================\n",
      "[[03/22/2019 10:13:11 PM]] Step 330: train 0.871369 lr: 1.723e-03\n",
      "[[03/22/2019 10:13:45 PM]] Step 360: train 0.783334 lr: 1.640e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:14:12 PM]] Snapshot loss 0.597297\n",
      "[[03/22/2019 10:14:20 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:14:20 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:14:20 PM]] ====================Epoch 7====================\n",
      "[[03/22/2019 10:14:45 PM]] Step 390: train 0.713684 lr: 1.557e-03\n",
      "[[03/22/2019 10:15:19 PM]] Step 420: train 0.657024 lr: 1.475e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 10:15:47 PM]] Snapshot loss 0.604756\n",
      "[[03/22/2019 10:15:47 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:15:47 PM]] ====================Epoch 8====================\n",
      "[[03/22/2019 10:16:12 PM]] Step 450: train 0.617289 lr: 1.392e-03\n",
      "[[03/22/2019 10:16:47 PM]] Step 480: train 0.580126 lr: 1.309e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:17:16 PM]] Snapshot loss 0.594027\n",
      "[[03/22/2019 10:17:24 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:17:25 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:17:25 PM]] ====================Epoch 9====================\n",
      "[[03/22/2019 10:17:49 PM]] Step 510: train 0.559856 lr: 1.226e-03\n",
      "[[03/22/2019 10:18:23 PM]] Step 540: train 0.536189 lr: 1.143e-03\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.34s/it]\n",
      "[[03/22/2019 10:18:54 PM]] Snapshot loss 0.581384\n",
      "[[03/22/2019 10:19:02 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:19:02 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:19:02 PM]] ====================Epoch 10====================\n",
      "[[03/22/2019 10:19:25 PM]] Step 570: train 0.511016 lr: 1.060e-03\n",
      "[[03/22/2019 10:19:59 PM]] Step 600: train 0.491929 lr: 9.767e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.32s/it]\n",
      "[[03/22/2019 10:20:29 PM]] Snapshot loss 0.594988\n",
      "[[03/22/2019 10:20:29 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:20:29 PM]] ====================Epoch 11====================\n",
      "[[03/22/2019 10:20:52 PM]] Step 630: train 0.479728 lr: 8.937e-04\n",
      "[[03/22/2019 10:21:26 PM]] Step 660: train 0.465072 lr: 8.108e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:21:58 PM]] Snapshot loss 0.588927\n",
      "[[03/22/2019 10:21:58 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:21:58 PM]] ====================Epoch 12====================\n",
      "[[03/22/2019 10:22:20 PM]] Step 690: train 0.454792 lr: 7.278e-04\n",
      "[[03/22/2019 10:22:53 PM]] Step 720: train 0.443796 lr: 6.448e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 10:23:26 PM]] Snapshot loss 0.581157\n",
      "[[03/22/2019 10:23:34 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:23:34 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:23:35 PM]] ====================Epoch 13====================\n",
      "[[03/22/2019 10:23:55 PM]] Step 750: train 0.437144 lr: 5.619e-04\n",
      "[[03/22/2019 10:24:29 PM]] Step 780: train 0.427699 lr: 4.789e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.29s/it]\n",
      "[[03/22/2019 10:25:02 PM]] Snapshot loss 0.579212\n",
      "[[03/22/2019 10:25:10 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:25:10 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:25:10 PM]] ====================Epoch 14====================\n",
      "[[03/22/2019 10:25:29 PM]] Step 810: train 0.415673 lr: 3.959e-04\n",
      "[[03/22/2019 10:26:04 PM]] Step 840: train 0.406198 lr: 3.130e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:26:38 PM]] Snapshot loss 0.585588\n",
      "[[03/22/2019 10:26:38 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:26:38 PM]] ====================Epoch 15====================\n",
      "[[03/22/2019 10:26:58 PM]] Step 870: train 0.403245 lr: 2.300e-04\n",
      "[[03/22/2019 10:27:30 PM]] Step 900: train 0.394558 lr: 1.470e-04\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:28:06 PM]] Snapshot loss 0.585125\n",
      "[[03/22/2019 10:28:06 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 8/8 [00:19<00:00,  2.30s/it]\n",
      "[[03/22/2019 10:28:26 PM]] Confirm val loss: 0.5792\n",
      "100%|██████████| 16/16 [01:26<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 3\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:30:20 PM]] SEED: 9293\n",
      "[[03/22/2019 10:30:20 PM]] # of paramters: 335,500,579\n",
      "[[03/22/2019 10:30:20 PM]] # of trainable paramters: 358,691\n",
      "[[03/22/2019 10:30:20 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/22/2019 10:30:20 PM]] Batches per epoch: 61\n",
      "[[03/22/2019 10:30:20 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:30:52 PM]] Step 30: train 1.606441 lr: 3.333e-04\n",
      "[[03/22/2019 10:31:26 PM]] Step 60: train 1.446130 lr: 5.833e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:31:44 PM]] Snapshot loss 0.893335\n",
      "[[03/22/2019 10:31:45 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:31:45 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:31:45 PM]] ====================Epoch 2====================\n",
      "[[03/22/2019 10:32:20 PM]] Step 90: train 1.282609 lr: 8.333e-04\n",
      "[[03/22/2019 10:32:53 PM]] Step 120: train 1.211982 lr: 1.083e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:33:13 PM]] Snapshot loss 0.719404\n",
      "[[03/22/2019 10:33:21 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:33:21 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:33:21 PM]] ====================Epoch 3====================\n",
      "[[03/22/2019 10:33:52 PM]] Step 150: train 1.133035 lr: 1.333e-03\n",
      "[[03/22/2019 10:34:27 PM]] Step 180: train 1.074981 lr: 1.583e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:34:48 PM]] Snapshot loss 0.630935\n",
      "[[03/22/2019 10:34:55 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:34:55 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:34:56 PM]] ====================Epoch 4====================\n",
      "[[03/22/2019 10:35:27 PM]] Step 210: train 1.013652 lr: 1.833e-03\n",
      "[[03/22/2019 10:36:01 PM]] Step 240: train 0.975458 lr: 1.972e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:36:22 PM]] Snapshot loss 0.668443\n",
      "[[03/22/2019 10:36:22 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:36:22 PM]] ====================Epoch 5====================\n",
      "[[03/22/2019 10:36:50 PM]] Step 270: train 0.935861 lr: 1.889e-03\n",
      "[[03/22/2019 10:37:24 PM]] Step 300: train 0.904479 lr: 1.806e-03\n",
      "100%|██████████| 8/8 [00:16<00:00,  1.83s/it]\n",
      "[[03/22/2019 10:37:46 PM]] Snapshot loss 0.629223\n",
      "[[03/22/2019 10:37:54 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:37:54 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:37:54 PM]] ====================Epoch 6====================\n",
      "[[03/22/2019 10:38:22 PM]] Step 330: train 0.798817 lr: 1.723e-03\n",
      "[[03/22/2019 10:38:55 PM]] Step 360: train 0.723318 lr: 1.640e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:39:19 PM]] Snapshot loss 0.621173\n",
      "[[03/22/2019 10:39:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:39:27 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:39:27 PM]] ====================Epoch 7====================\n",
      "[[03/22/2019 10:39:53 PM]] Step 390: train 0.677837 lr: 1.557e-03\n",
      "[[03/22/2019 10:40:28 PM]] Step 420: train 0.632042 lr: 1.475e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:40:53 PM]] Snapshot loss 0.617457\n",
      "[[03/22/2019 10:41:01 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:41:01 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:41:02 PM]] ====================Epoch 8====================\n",
      "[[03/22/2019 10:41:26 PM]] Step 450: train 0.599768 lr: 1.392e-03\n",
      "[[03/22/2019 10:42:03 PM]] Step 480: train 0.567696 lr: 1.309e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:42:29 PM]] Snapshot loss 0.606661\n",
      "[[03/22/2019 10:42:37 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:42:37 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:42:37 PM]] ====================Epoch 9====================\n",
      "[[03/22/2019 10:43:02 PM]] Step 510: train 0.548251 lr: 1.226e-03\n",
      "[[03/22/2019 10:43:37 PM]] Step 540: train 0.521749 lr: 1.143e-03\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:44:04 PM]] Snapshot loss 0.606608\n",
      "[[03/22/2019 10:44:12 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:44:12 PM]] ====================Epoch 10====================\n",
      "[[03/22/2019 10:44:36 PM]] Step 570: train 0.501539 lr: 1.060e-03\n",
      "[[03/22/2019 10:45:10 PM]] Step 600: train 0.486453 lr: 9.767e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:45:38 PM]] Snapshot loss 0.620684\n",
      "[[03/22/2019 10:45:38 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:45:38 PM]] ====================Epoch 11====================\n",
      "[[03/22/2019 10:46:02 PM]] Step 630: train 0.470656 lr: 8.937e-04\n",
      "[[03/22/2019 10:46:37 PM]] Step 660: train 0.457673 lr: 8.108e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:47:06 PM]] Snapshot loss 0.599227\n",
      "[[03/22/2019 10:47:14 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:47:14 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:47:15 PM]] ====================Epoch 12====================\n",
      "[[03/22/2019 10:47:36 PM]] Step 690: train 0.447866 lr: 7.278e-04\n",
      "[[03/22/2019 10:48:09 PM]] Step 720: train 0.432134 lr: 6.448e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:48:40 PM]] Snapshot loss 0.619829\n",
      "[[03/22/2019 10:48:40 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:48:40 PM]] ====================Epoch 13====================\n",
      "[[03/22/2019 10:49:02 PM]] Step 750: train 0.420699 lr: 5.619e-04\n",
      "[[03/22/2019 10:49:34 PM]] Step 780: train 0.412206 lr: 4.789e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:50:07 PM]] Snapshot loss 0.604588\n",
      "[[03/22/2019 10:50:07 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:50:07 PM]] ====================Epoch 14====================\n",
      "[[03/22/2019 10:50:26 PM]] Step 810: train 0.406638 lr: 3.959e-04\n",
      "[[03/22/2019 10:50:59 PM]] Step 840: train 0.395669 lr: 3.130e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:51:33 PM]] Snapshot loss 0.611135\n",
      "[[03/22/2019 10:51:33 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:51:33 PM]] ====================Epoch 15====================\n",
      "[[03/22/2019 10:51:50 PM]] Step 870: train 0.388558 lr: 2.300e-04\n",
      "[[03/22/2019 10:52:24 PM]] Step 900: train 0.375997 lr: 1.470e-04\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:52:59 PM]] Snapshot loss 0.616482\n",
      "[[03/22/2019 10:52:59 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 8/8 [00:17<00:00,  1.88s/it]\n",
      "[[03/22/2019 10:53:18 PM]] Confirm val loss: 0.5992\n",
      "100%|██████████| 16/16 [01:26<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 4\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:55:12 PM]] SEED: 9293\n",
      "[[03/22/2019 10:55:12 PM]] # of paramters: 335,500,579\n",
      "[[03/22/2019 10:55:12 PM]] # of trainable paramters: 358,691\n",
      "[[03/22/2019 10:55:12 PM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.002\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/22/2019 10:55:12 PM]] Batches per epoch: 61\n",
      "[[03/22/2019 10:55:12 PM]] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/22/2019 10:55:42 PM]] Step 30: train 1.525416 lr: 3.333e-04\n",
      "[[03/22/2019 10:56:16 PM]] Step 60: train 1.391759 lr: 5.833e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 10:56:36 PM]] Snapshot loss 0.878458\n",
      "[[03/22/2019 10:56:37 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:56:37 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:56:38 PM]] ====================Epoch 2====================\n",
      "[[03/22/2019 10:57:13 PM]] Step 90: train 1.249881 lr: 8.333e-04\n",
      "[[03/22/2019 10:57:46 PM]] Step 120: train 1.163046 lr: 1.083e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 10:58:07 PM]] Snapshot loss 0.720036\n",
      "[[03/22/2019 10:58:15 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:58:15 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:58:15 PM]] ====================Epoch 3====================\n",
      "[[03/22/2019 10:58:46 PM]] Step 150: train 1.105200 lr: 1.333e-03\n",
      "[[03/22/2019 10:59:21 PM]] Step 180: train 1.050429 lr: 1.583e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 10:59:44 PM]] Snapshot loss 0.699233\n",
      "[[03/22/2019 10:59:52 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 10:59:52 PM]] New low\n",
      "\n",
      "[[03/22/2019 10:59:52 PM]] ====================Epoch 4====================\n",
      "[[03/22/2019 11:00:21 PM]] Step 210: train 1.000802 lr: 1.833e-03\n",
      "[[03/22/2019 11:00:55 PM]] Step 240: train 0.958700 lr: 1.972e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:01:19 PM]] Snapshot loss 0.616945\n",
      "[[03/22/2019 11:01:27 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:01:27 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:01:27 PM]] ====================Epoch 5====================\n",
      "[[03/22/2019 11:01:56 PM]] Step 270: train 0.922242 lr: 1.889e-03\n",
      "[[03/22/2019 11:02:29 PM]] Step 300: train 0.887584 lr: 1.806e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:02:54 PM]] Snapshot loss 0.584092\n",
      "[[03/22/2019 11:03:02 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:03:02 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:03:02 PM]] ====================Epoch 6====================\n",
      "[[03/22/2019 11:03:29 PM]] Step 330: train 0.789545 lr: 1.723e-03\n",
      "[[03/22/2019 11:04:01 PM]] Step 360: train 0.719608 lr: 1.640e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:04:28 PM]] Snapshot loss 0.573157\n",
      "[[03/22/2019 11:04:36 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:04:36 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:04:36 PM]] ====================Epoch 7====================\n",
      "[[03/22/2019 11:05:02 PM]] Step 390: train 0.675218 lr: 1.557e-03\n",
      "[[03/22/2019 11:05:37 PM]] Step 420: train 0.634343 lr: 1.475e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:06:04 PM]] Snapshot loss 0.554331\n",
      "[[03/22/2019 11:06:12 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:06:12 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:06:12 PM]] ====================Epoch 8====================\n",
      "[[03/22/2019 11:06:36 PM]] Step 450: train 0.595962 lr: 1.392e-03\n",
      "[[03/22/2019 11:07:10 PM]] Step 480: train 0.567157 lr: 1.309e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:07:38 PM]] Snapshot loss 0.560226\n",
      "[[03/22/2019 11:07:38 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:07:38 PM]] ====================Epoch 9====================\n",
      "[[03/22/2019 11:08:04 PM]] Step 510: train 0.543304 lr: 1.226e-03\n",
      "[[03/22/2019 11:08:38 PM]] Step 540: train 0.521663 lr: 1.143e-03\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:09:07 PM]] Snapshot loss 0.563015\n",
      "[[03/22/2019 11:09:07 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:09:07 PM]] ====================Epoch 10====================\n",
      "[[03/22/2019 11:09:31 PM]] Step 570: train 0.498123 lr: 1.060e-03\n",
      "[[03/22/2019 11:10:04 PM]] Step 600: train 0.483016 lr: 9.767e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:10:34 PM]] Snapshot loss 0.542090\n",
      "[[03/22/2019 11:10:42 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:10:42 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:10:42 PM]] ====================Epoch 11====================\n",
      "[[03/22/2019 11:11:04 PM]] Step 630: train 0.467448 lr: 8.937e-04\n",
      "[[03/22/2019 11:11:39 PM]] Step 660: train 0.450782 lr: 8.108e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:12:10 PM]] Snapshot loss 0.539893\n",
      "[[03/22/2019 11:12:18 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:12:18 PM]] New low\n",
      "\n",
      "[[03/22/2019 11:12:18 PM]] ====================Epoch 12====================\n",
      "[[03/22/2019 11:12:39 PM]] Step 690: train 0.436039 lr: 7.278e-04\n",
      "[[03/22/2019 11:13:13 PM]] Step 720: train 0.426830 lr: 6.448e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:13:45 PM]] Snapshot loss 0.550361\n",
      "[[03/22/2019 11:13:45 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:13:45 PM]] ====================Epoch 13====================\n",
      "[[03/22/2019 11:14:07 PM]] Step 750: train 0.414986 lr: 5.619e-04\n",
      "[[03/22/2019 11:14:40 PM]] Step 780: train 0.402344 lr: 4.789e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:15:13 PM]] Snapshot loss 0.540553\n",
      "[[03/22/2019 11:15:13 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:15:13 PM]] ====================Epoch 14====================\n",
      "[[03/22/2019 11:15:32 PM]] Step 810: train 0.395119 lr: 3.959e-04\n",
      "[[03/22/2019 11:16:05 PM]] Step 840: train 0.384625 lr: 3.130e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:16:41 PM]] Snapshot loss 0.540454\n",
      "[[03/22/2019 11:16:41 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:16:41 PM]] ====================Epoch 15====================\n",
      "[[03/22/2019 11:16:59 PM]] Step 870: train 0.383783 lr: 2.300e-04\n",
      "[[03/22/2019 11:17:32 PM]] Step 900: train 0.376357 lr: 1.470e-04\n",
      "100%|██████████| 8/8 [00:18<00:00,  2.34s/it]\n",
      "[[03/22/2019 11:18:08 PM]] Snapshot loss 0.533004\n",
      "[[03/22/2019 11:18:16 PM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/22/2019 11:18:16 PM]] New low\n",
      "\n",
      "100%|██████████| 8/8 [00:17<00:00,  2.27s/it]\n",
      "[[03/22/2019 11:18:35 PM]] Confirm val loss: 0.5330\n",
      "100%|██████████| 16/16 [01:26<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=191)\n",
    "\n",
    "val_preds, test_preds, val_ys, val_losses = [], [], [], []\n",
    "for train_index, valid_index in skf.split(df_train, df_train[\"target\"]):\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Fold \" + str(len(val_preds)))\n",
    "    print(\"=\" * 20)\n",
    "    train_ds = GAPDataset(df_train.iloc[train_index], tokenizer)\n",
    "    val_ds = GAPDataset(df_train.iloc[valid_index], tokenizer)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        collate_fn = collate_examples,\n",
    "        batch_size=64,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n",
    "    # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n",
    "    set_trainable(model.bert, False)\n",
    "    set_trainable(model.head, True)\n",
    "    optimizer = WeightDecayOptimizerWrapper(\n",
    "        torch.optim.Adam(model.parameters(), lr=2e-3),\n",
    "        0.05\n",
    "    )\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    bot = GAPBot(\n",
    "        model, train_loader, val_loader,\n",
    "        optimizer=optimizer, echo=True,\n",
    "        avg_window=40\n",
    "    )\n",
    "    gc.collect()\n",
    "    steps_per_epoch = len(train_loader) \n",
    "    n_steps = steps_per_epoch * 15\n",
    "    bot.train(\n",
    "        n_steps,\n",
    "        log_interval=steps_per_epoch // 2,\n",
    "        snapshot_interval=steps_per_epoch,\n",
    "#         scheduler=GradualWarmupScheduler(optimizer, 20, int(steps_per_epoch * 4),\n",
    "#             after_scheduler=CosineAnnealingLR(\n",
    "#                 optimizer, n_steps - int(steps_per_epoch * 4)\n",
    "#             )\n",
    "#         )\n",
    "        scheduler=TriangularLR(\n",
    "            optimizer, 20, ratio=3, steps_per_cycle=n_steps)\n",
    "    )\n",
    "    # Load the best checkpoint\n",
    "    bot.load_model(bot.best_performers[0][1])\n",
    "    bot.remove_checkpoints(keep=0)    \n",
    "    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
    "    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n",
    "    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n",
    "    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n",
    "    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.74793327, 0.2143201 , 0.03774654],\n",
       "        [0.9880679 , 0.00988253, 0.0020495 ],\n",
       "        [0.02281776, 0.6646731 , 0.3125091 ],\n",
       "        ...,\n",
       "        [0.46640489, 0.28172353, 0.25187162],\n",
       "        [0.9913982 , 0.00413572, 0.00446608],\n",
       "        [0.0613956 , 0.3166686 , 0.6219358 ]], dtype=float32),\n",
       " array([[0.17300756, 0.73620236, 0.09079007],\n",
       "        [0.9953689 , 0.00311402, 0.00151702],\n",
       "        [0.04477216, 0.8251235 , 0.13010427],\n",
       "        ...,\n",
       "        [0.18542829, 0.68099666, 0.13357507],\n",
       "        [0.9867958 , 0.0096134 , 0.00359084],\n",
       "        [0.04641515, 0.84494936, 0.10863551]], dtype=float32),\n",
       " array([[0.66663164, 0.24300736, 0.09036099],\n",
       "        [0.98768544, 0.00881832, 0.00349617],\n",
       "        [0.03495398, 0.8585785 , 0.10646757],\n",
       "        ...,\n",
       "        [0.3258655 , 0.5802075 , 0.09392688],\n",
       "        [0.99470913, 0.00337524, 0.00191567],\n",
       "        [0.15055966, 0.7541962 , 0.0952441 ]], dtype=float32),\n",
       " array([[4.8624769e-01, 3.6400113e-01, 1.4975119e-01],\n",
       "        [9.6186429e-01, 3.4138564e-02, 3.9971429e-03],\n",
       "        [1.1619809e-02, 8.6843717e-01, 1.1994297e-01],\n",
       "        ...,\n",
       "        [2.2075626e-01, 4.6711105e-01, 3.1213275e-01],\n",
       "        [9.9544865e-01, 7.7056006e-04, 3.7808884e-03],\n",
       "        [7.5302631e-02, 6.2776345e-01, 2.9693395e-01]], dtype=float32),\n",
       " array([[0.5683532 , 0.22112599, 0.21052082],\n",
       "        [0.9917293 , 0.00562502, 0.00264562],\n",
       "        [0.0148143 , 0.7651104 , 0.22007531],\n",
       "        ...,\n",
       "        [0.6973515 , 0.11467323, 0.18797523],\n",
       "        [0.99103534, 0.00610111, 0.0028635 ],\n",
       "        [0.04786777, 0.86985594, 0.08227631]], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5772331131093703,\n",
       " 0.5470178227560701,\n",
       " 0.5792128030876269,\n",
       " 0.5992271633830719,\n",
       " 0.5330046828203046]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_preds = np.mean(test_preds, axis=0)\n",
    "final_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5343874468779977"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(df_test.target, final_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528435</td>\n",
       "      <td>0.355731</td>\n",
       "      <td>0.115834</td>\n",
       "      <td>development-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.984943</td>\n",
       "      <td>0.012316</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>development-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.796385</td>\n",
       "      <td>0.177820</td>\n",
       "      <td>development-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.569177</td>\n",
       "      <td>0.398956</td>\n",
       "      <td>development-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.980349</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>development-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B   NEITHER             ID\n",
       "0  0.528435  0.355731  0.115834  development-1\n",
       "1  0.984943  0.012316  0.002741  development-2\n",
       "2  0.025796  0.796385  0.177820  development-3\n",
       "3  0.031866  0.569177  0.398956  development-4\n",
       "4  0.007956  0.980349  0.011695  development-5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission file\n",
    "df_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\n",
    "df_sub[\"ID\"] = df_test.ID\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "df_sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
